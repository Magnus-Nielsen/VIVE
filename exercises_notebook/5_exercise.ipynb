{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 5: Unsupervised learning & Text as data\n",
    "\n",
    "In this exercise set, we will be looking at:\n",
    "\n",
    "1. Unsupervised learning, focusing on the canonical `Principal Component Analysis` and `K-means` for dimensionality reduction and clustering, respectively\n",
    "2. Text as data, focusing on `VADER` and `bag-of-words` models\n",
    "\n",
    "The focus in the first part is implementing the methods using `sklearn` and then how we can use and evaluate these methods. In the second part, we see how we can use text as both unsupervised input to dictionary based methods, but also how the more general `bag-of-words` models allow us to use text as regular tabular input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "The dataset we will be looking at this time is the *UCI ML Wine recognition dataset*. This features analysis of 178 wines from three different wine manufacturers, and as it is often used you will be able to find examples analyzing this online. Furthermore, this entails that we have a ground truth for our clustering algorithms, which is nice to know when getting started with clustering. As last time, you're welcome to use a dataset of your own.\n",
    "\n",
    "**Load data**\n",
    "\n",
    "Here we load our input data into a `DataFrame` called `X` and our target data into a `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T12:18:56.210482Z",
     "start_time": "2020-02-25T12:18:55.219063Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Get wine data\n",
    "data_wine = load_wine(as_frame=True)\n",
    "X = data_wine.data\n",
    "y = data_wine.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe the data using both the documentation which came with the data, but also by computing summary statistics for the input data and value counts for the target. \n",
    "\n",
    "Consider whether the input features are measured on the same scale and whether the classes heavily skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "As we saw, the data has 13 dimensions, and the goal of this section is to reduce this to a lower amount of dimensions. \n",
    "\n",
    "This can be done for many reasons, including:\n",
    "\n",
    "- Reduce computation time\n",
    "- Performance increases\n",
    "- Visualization\n",
    "\n",
    "This we will do using principal component analysis. All the same things regarding data leakage from train to test data carries over from supervised learning, but we will disregard this aspect and use all data at once for simplicity. Later on, it can be used in a step in your pipelines, and it will only learn from the train data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.1**\n",
    "> \n",
    "> Fill in the missing code to perform a principal component analysis using `sklearn`\n",
    ">\n",
    "> *Hints:*\n",
    ">> Were all the variables on the same scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import # FILL IN\n",
    "from sklearn.decomposition import # FILL IN\n",
    "\n",
    "# Step one\n",
    "sc = # FILL IN\n",
    "sc.fit(X)\n",
    "X_std = sc.transform(X)\n",
    "\n",
    "# Step two\n",
    "pca = # FILL IN\n",
    "pca.fit(X_std)\n",
    "X_pca = pca.transform(X_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.2**\n",
    "> \n",
    "> 1. What are the dimensions of `X_pca`? \n",
    "> 2. Have you reduced the dimensionality?\n",
    ">\n",
    "> **Hints:**\n",
    ">> The shape of an array can be determined using `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.3**\n",
    "> \n",
    "> Plot the two first principal components in a scatter plot by filling in the missing code\n",
    ">\n",
    "> **Hints:**\n",
    ">> When subsetting arrays, the first input determines the rows and the second determines columns\n",
    ">>\n",
    ">> The two inputs are separated by a comma\n",
    ">> \n",
    ">> The input `:` corresponds to all\n",
    ">>\n",
    ">> Python is zero-index, i.e. `0` corresponds to the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.scatter(X_pca[FILL IN], X_pca[FILL IN]) # Missing code\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.4**\n",
    "> \n",
    "> Reuse the code from before, but add colors by adding the option `c = y` to the scatter plot.\n",
    "> Can we see a difference between the three wine cultivators?\n",
    ">\n",
    "> **Hints:**\n",
    ">> This colors the plot according to the class of the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have chosen two dimensions for visualisation, but sometimes we might want to make a more informed choice about the amount of dimensions based on the variance kept or lost. This information can be obtained using a scree plot.\n",
    "\n",
    "To create the scree plot, we need to calculate the explained variance ratio for each principal component.\n",
    "\n",
    "Implementing stuff on your own might cause entail minor bugs and errors. Perhaps `sklearn` has an implementation for us?\n",
    "\n",
    "> **Exercise 1.5**\n",
    "> \n",
    "> Look at the documentation for the [PCA function](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). \n",
    "> - Does it have a feature/attribute which calculates it for us? \n",
    "> - How would we access this feature?\n",
    ">\n",
    "> **Hints:**\n",
    ">> Look under *Attributes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.6**\n",
    "> 1. Extract the explained variance ratio\n",
    "> 2. Calculate the cumulative explained variance ratio\n",
    "> \n",
    "> Hints: \n",
    ">> Attributes can be accessed using a period (`.`)\n",
    ">>\n",
    ">> `numpy` has a function for calculating cumulative sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.7**\n",
    "> \n",
    "> Create a scree plot using the code below, inserting the appropriate x and y variables\n",
    "> \n",
    "> Hints: \n",
    ">> `PC_values` is an array that goes from `1` to `13`, which corresponds to the amount of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.bar(FILL IN)\n",
    "plt.step(FILL IN)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to decide on the amount of dimensions, most often through cross validation, compute constraints, or a heuristic such as the elbow method.\n",
    "\n",
    "However, as we are going to continue to plotting the data in a 2-dimensional space, we only need two principal components.\n",
    "\n",
    "It seems superfluous to return all the principal components, doesn't it?\n",
    "\n",
    "> **Exercise 1.8**\n",
    "> \n",
    "> Change the code from exercise 1.1 to only return the first two components\n",
    "> \n",
    "> Call the transformed data `X_pca_2`\n",
    "> \n",
    "> Hints: \n",
    ">> `PCA` has an input which decides the amount of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Having now performed dimensionality reduction, we will use the `K-means` algorithm to cluster the data. In this case, we know that three classes exist, but `K-means` will not use this information.\n",
    "\n",
    "First we implement the method, and then we continue to look at how one can evaluate the method and choose the amount of clusters.\n",
    "\n",
    "There are many other clustering methods, and if you want to use other methods, a starting point could be the [clustering section in sklearn](https://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.1**\n",
    "> \n",
    "> Fill in the missing code such that you implement a `K-means` clustering algorithm with three clusters.\n",
    "> For replicability, you should also set a random state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import \n",
    "\n",
    "# fit the pca and get the two first components\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_pca = PCA(n_components=2).fit_transform(X_std)\n",
    "\n",
    "# apply the \n",
    "kmeans = FILL IN\n",
    "kmeans.fit(X_pca)\n",
    "y_kmeans = kmeans.predict(X_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualizes the found clusters from the previous exercise.\n",
    "\n",
    "> **Exercise 2.2**\n",
    "> \n",
    "> Explain the code by filling in the missing comments, one at each `#`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_kmeans = pd.DataFrame(X_pca)\n",
    "X_kmeans['cluster_id'] = y_kmeans\n",
    "\n",
    "#\n",
    "unique_cluster_ids = X_kmeans['cluster_id'].unique()\n",
    "\n",
    "#\n",
    "for cluster_id in unique_cluster_ids:\n",
    "    #\n",
    "    cluster_subset = X_kmeans.loc[X_kmeans.cluster_id == cluster_id]\n",
    "    #\n",
    "    plt.scatter(cluster_subset[0], cluster_subset[1])\n",
    "    \n",
    "#\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "#\n",
    "plt.scatter(centroids[:,0], centroids[:,1], c='black', s=80)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have chosen three dimensions because I told you to, but usually you would have to decide upon this yourself, a downside of `K-means`. \n",
    "\n",
    "To assist us, we can look for elbows in what the model optimizes.\n",
    "\n",
    "> **Exercise 2.3**\n",
    "> \n",
    "> The `K-means` algorithm minimizes the sum of squared distances to the nearest centroid. \n",
    "> This is available through the `Kmeans` object. Look through the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to find out how to extract this information.\n",
    "> Using this knowledge, fill in the missing code to plot the sum of squared distances for 1 to 10 clusters.\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> Try looking under *Attributes*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_range = range(1, FILL IN)\n",
    "sum_squared_distances_list = []\n",
    "\n",
    "# For each cluster, calculate sum of squared distances\n",
    "for no_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=no_clusters, random_state=73)\n",
    "    kmeans.fit(X_pca)\n",
    "    sum_squared_distances_list.append(FILL IN)\n",
    "\n",
    "\n",
    "# Plot the sum of squared distances as a function of cluster range\n",
    "plt.plot(cluster_range, sum_squared_distances_list, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are many different metrics to evaluate a clustering algorithm. A list of those implemented in `sklearn` can be found in their [user guide](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation), which also includes pros and cons of each metric. \n",
    "\n",
    "> **Exercise 2.4**\n",
    "> \n",
    "> The code below calculates the average silhoutte coefficient, see [documentation here](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient).\n",
    ">\n",
    "> 1. What is the range of values, and what values are preferred? \n",
    "> 2. Should one be wary of using this method to compare across models from the three broad categories introduced in the lecture?\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> Think about convexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "clusterer = KMeans(n_clusters=3, random_state=73)\n",
    "cluster_labels = clusterer.fit_predict(X_pca)\n",
    "silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "print(f\"Average silhouette coefficient: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having now seen how to calculate the silhouette coefficient, we want to look at how it varies with up to ten clusters\n",
    "\n",
    "> **Exercise 2.5**\n",
    "> \n",
    "> Fill in the missing code to calculate the average silhouette coefficients\n",
    ">\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> How many clusters are needed to calculate the silhouette coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Specify range of clusters\n",
    "cluster_range_silhouette = range(FILL IN)\n",
    "avg_silhouette_list = []\n",
    "\n",
    "# Calculate the average silhouette coefficient\n",
    "for no_clusters in cluster_range_silhouette:\n",
    "    kmeans = KMeans(n_clusters=no_clusters, random_state=73)\n",
    "    kmeans.fit(X_pca)\n",
    "    cluster_labels = kmeans.predict(X_pca)\n",
    "    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "    avg_silhouette_list.append(silhouette_avg)\n",
    "\n",
    "# Plot average silhouette coefficients\n",
    "plt.plot(FILL IN, FILL IN, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average silhouette coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make silhouette plots, although they are a bit tedious to produce. Code to produce it using just `sklearn` can be found online, but there also exist packages to do it for us! `yellowbrick` is one such package, and it even uses the same syntax as `sklearn`. As a general rule, it's always a good idea to check if there exists a package which does what you want to do, ideally before you spend too much time implementing stuff.\n",
    "\n",
    "> **Exercise 2.6**\n",
    "> \n",
    "> Install the package `yellowbrick` to plot the silhoutte plot using the code below. \n",
    ">\n",
    "> Bonus: Try plotting different amounts of cluster amounts. Which amount do you prefer?\n",
    ">\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> Installing with `pip` follows standard naming conventions, but otherwise installation instructions can be found on their website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Model we want to evaluate\n",
    "kmeans = KMeans(n_clusters=3, random_state=73)\n",
    "\n",
    "# The vizualiser\n",
    "visualizer = SilhouetteVisualizer(kmeans)\n",
    "\n",
    "# Fit the data to the visualizer\n",
    "visualizer.fit(X_pca)       \n",
    "\n",
    "# Show the plot\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text as data\n",
    "\n",
    "The dataset we will be looking at to get used to working with text as data is [IMDB Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) downloaded from Kaggle, but originally from [Stanford](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "and created for the paper *Maas, Andrew, et al. \"Learning word vectors for sentiment analysis.\" Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. 2011.*\n",
    "\n",
    "The dataset consists of 50.000 movie reviews, which are humanly classified as either positive or negative (25.000 of each).\n",
    "\n",
    "**Load data**\n",
    "\n",
    "Here we load our data into a `DataFrame` called `df`. Furthermore, we map the classes into a binary vector which indicates whether the review was positive (`1`) or negative (`0`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('movie_data.csv.zip', encoding='utf-8', compression='zip')\n",
    "df['positive'] = df['sentiment'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sensible first thing to do is to read some of the text. The code below does enables you to do this, printing the first two positive and negative reviews. \n",
    "\n",
    "> **Exercise 3.1**\n",
    "> \n",
    "> Are there any weird artifacts in the text? If there are any, can you guess why they're there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive\")\n",
    "print()\n",
    "for i in df.loc[df.sentiment == 'positive'].review[:2]:\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "print(\"Negative\")\n",
    "print()\n",
    "for i in df.loc[df.sentiment == 'negative'].review[:2]:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a dataset with labels is not always easy. If we had no labels but were still interested in the sentiment of the reviews, one way to go about this would be using a dictionary based method. \n",
    "\n",
    "In this example, we will use the `VADER` sentiment analyser to get the sentiment of the reviews.\n",
    "\n",
    "> **Exercise 3.2**\n",
    "> \n",
    "> Explain what happens in each of the four steps by commenting the code. \n",
    ">\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> `.apply` applies a function to the column\n",
    ">>\n",
    ">> `lambda` functions are anonymous function which are defined inplace. In this situation, they are applied to each row in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#\n",
    "df['scores'] = df['review'].apply(lambda review: sia.polarity_scores(review))\n",
    "\n",
    "#\n",
    "df['compound'] = df['scores'].apply(lambda scores: scores['compound'])\n",
    "\n",
    "#\n",
    "df['comp_score'] = df['compound'].apply(lambda comp_score: 1 if comp_score >= 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# create instance\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate the scores\n",
    "df['scores'] = df['review'].apply(lambda review: sia.polarity_scores(review))\n",
    "\n",
    "# Extract the compound score (-1 to 1)\n",
    "df['compound'] = df['scores'].apply(lambda scores: scores['compound'])\n",
    "\n",
    "# Turn it into a binary variable signalling positive (1) or negative (0)\n",
    "df['comp_score'] = df['compound'].apply(lambda comp_score: 1 if comp_score >= 0 else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are so lucky to have a labelled dataset, we can see how our unsupervised method did!\n",
    "\n",
    "> **Exercise 3.3**\n",
    "> \n",
    "> Calculate the accuracy of the predicted `comp_score` (compound scores)\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> Try importing `accuracy_score` from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VADER` is relatively advanced, and uses information about whether the text is capitalized and uses exclamation marks. However, for `bag-of-words` models and other text models, it is common to preprocess the data to reduce the complexity. \n",
    "\n",
    "In the following code, I give you some examples of how one could preprocess the data. One of the common tools used is Regular Expressions, shortened `re`. I do not expect you to know it, but it's a neat tool for capturing text and either storing it or replacing it with other text. You can play around with it at [RegExr.com](https://regexr.com/), should you wish. \n",
    "\n",
    "> **Exercise 3.4**\n",
    "> \n",
    "> Look at the reviews after each cleaning example. What's the difference between the two preprocessing methods? Is the text better represented than before we preprocessed it?\n",
    "> Some things you could consider:\n",
    "> - Does it make the text more readable for you? What about for an algorithm?  \n",
    "> - Have we removed the weird artifacts you (perhaps) found earlier?\n",
    "> - Have we introduced any new weird artifacts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean reviews\n",
    "def cleaner(document):\n",
    "    document = document.lower() #To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document) #Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s]','', document) #Remove non-alphanumeric characters\n",
    "    return document\n",
    "\n",
    "df['review_clean'] = df['review'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive\")\n",
    "print()\n",
    "for i in df.loc[df.sentiment == 'positive'].review_clean[:2]:\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "print(\"Negative\")\n",
    "print()\n",
    "for i in df.loc[df.sentiment == 'negative'].review_clean[:2]:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Extended cleaning function\n",
    "def extended_cleaner(document, stopwords_list = english_stopwords):\n",
    "    document = document.lower() # To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document) # Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s]','', document) # Remove non-alphanumeric characters\n",
    "    text = ' '.join(x for x in document.split(' ') if x not in stopwords_list) # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df['review_extended_clean'] = df['review'].apply(extended_cleaner)# Clean reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive\")\n",
    "print()\n",
    "for i in df.loc[df.sentiment == 'positive'].review_extended_clean[:2]:\n",
    "    print(i)\n",
    "    print()\n",
    "\n",
    "print(\"Negative\")\n",
    "print()\n",
    "for i in df.loc[df.sentiment == 'negative'].review_extended_clean[:2]:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having now preprocessed the text, we want to implement a `bag-of-words` model.\n",
    "\n",
    "> **Exercise 3.5**\n",
    "> \n",
    "> Implement a model that count the amount of unique words in each sentence by filling in the missing code\n",
    ">> *Hints:*\n",
    ">>\n",
    ">> Try importing `CountVectorizer`\n",
    ">>\n",
    ">> It has a method which both fits and transforms the data in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import FILL IN\n",
    "\n",
    "vectorizer = FILL IN\n",
    "\n",
    "X = df.review_extended_clean\n",
    "\n",
    "X_bag = FILL IN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.6**\n",
    "> \n",
    "> We have now vectorized the text, and have a variable called `X_bag`.\n",
    ">\n",
    "> 1. What is the type of `X_bag`?\n",
    "> 2. What is the dimensionality of `X_bag`?\n",
    "> 3. Could we use simple unregularized linear regression with this input?\n",
    "> \n",
    ">> *Hints:*\n",
    ">>\n",
    ">> How many samples compared to variables do we have?\n",
    ">>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having now seen the workings of the `CountVectorizer`, we're going to implement it in a pipeline so it can be used for supervised learning as we have seen whilst avoiding data leakage. We do not perform cross validation to reduce the time it takes to run.\n",
    "\n",
    "> **Exercise 3.7**\n",
    "> \n",
    "> Fill in the missing code such that we implement a `CountVectorizer` followed by a `LogisticRegression`.\n",
    ">\n",
    "> Does it perform better than `VADER`?\n",
    ">> *Hints:*\n",
    ">> \n",
    ">> We have previously looked at pipelines and datasplitting. Try looking at last sessions exercises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df.positive\n",
    "X = df.review_extended_clean\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(FILL IN, test_size=0.3, random_state=73)\n",
    "\n",
    "\n",
    "tf_clf = Pipeline([FILL IN])\n",
    "\n",
    "tf_clf.fit(X_train, y_train)\n",
    "tf_acc = tf_clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {tf_acc:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.8**\n",
    "> \n",
    "> Change the vectorizer from the previous exercise to a tf-idf vectorizer followed by a `LogisticRegression`.\n",
    ">\n",
    "> Does the model perform better?\n",
    ">> *Hints:*\n",
    ">> \n",
    ">> Try googling `sklearn tfidf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now looked at some ways of how to work with text. You could also look into:\n",
    "\n",
    "- Stemming and lemmatization\n",
    "- N-gram models (both vectorizers support it)\n",
    "- Changing the minimum or maximum frequency that words need to appear with\n",
    "\n",
    "Another model to look into that is not too computationally difficult is topic models. \n",
    "\n",
    "A cool application of topic models can be seen in [Transparency and Deliberation within the FOMC: A Computational Linguistics Approach](https://sekhansen.github.io/pdf_files/qje_2018.pdf), with the most information about the text analysis in section IV. \n",
    "\n",
    "`sklearn` has an implementation of a LDA topic model ([sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)), although it is my impression that it is most commonly done using `gensim`, see their website [here](https://radimrehurek.com/gensim/index.html#)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vive_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "50f1b660881291c5d31ad4d40c48915b3ef9364cc881d538585b11a7eb841304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
