{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 4: Supervised learning\n",
    "\n",
    "In this exercise set, we will mainly be looking at different supervised learning algorithms, both tinkering around with them and seeing how the models perform for a given dataset. We will look at:\n",
    "\n",
    "- Logistic regression\n",
    "- Decision tree\n",
    "- Ensemble methods\n",
    "  - Random Forest\n",
    "  - AdaBoost\n",
    "- Neural network\n",
    "\n",
    "If you in general need more information about models or how to tune their hyperparameters, try looking up the documentation or googling `hyperparameter tuning + model_name`\n",
    "\n",
    "\n",
    "Throughout your career, you've probably worked with many problems. Some problems can easily be formulated as regression problem, whereas others are easily formulated as a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.1**\n",
    "> \n",
    "> Name three different problems which you've worked with where the outcome of interest was:\n",
    ">\n",
    "> - Continuous (regression) \n",
    "> \n",
    "> - Categorical (classification)\n",
    ">   \n",
    "> Have you encountered problems where the outcome of interest could be both continuous and categorical?\n",
    "> Would being able to predict these outcomes of interest be valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this session, I invite you to use a dataset of your own, as different models work best for different problems:\n",
    "- This can be either a regression problem or a classification problem. \n",
    "- Feel free to preprocess in another program and export it as a `csv` file or another format of your choosing\n",
    "\n",
    "The exercises are designed with a classification problem in mind, but all exercises except the ones about confusion matrices can be exchanged for regression problems by changin from a `Classifier` to a `Regressor` model.\n",
    "\n",
    "The dataset I've decided upon is a dataset regarding classification of high income people, namely the [Census Income Data Set from the UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Census+Income). I've reduced the amount of features and sample size, as well as done a little bit of cleaning, from the full sample to reduce computation time. All the categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress convergencewarnings if they appear\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Actual code to load\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('adult_preprocessed.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.2**\n",
    "> \n",
    "> What column in the DataFrame is the target of interest? Subset this as a `Series` called `y`, and the rest of the columns as a `DataFrame` called `X`\n",
    "> \n",
    ">> *Hints:* \n",
    ">> \n",
    ">> `y = df['column_name']` subsets a column as a Series.\n",
    ">>\n",
    ">> `X = df.drop(columns='column_name')` drops a column in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.3**\n",
    "> \n",
    "> As a first step, you should split the data into a development and test set. Make a development and test split with 80% of the data in the development set. Name them `X_dev`, `X_test`, `y_dev` and `y_test`\n",
    "> \n",
    ">> *Hints:*\n",
    ">> \n",
    ">> Try importing `train_test_split` from `sklearn.model_selection`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation curves\n",
    "\n",
    "Last week, you were introduced to validation curves. This is a way of getting an understanding how a single hyperparameter changes the performance of a model on both seen and unseen data. We will be using this tool throughout these exercises to probe the models and see how the hyperparameters change the performance of the model.\n",
    "\n",
    "Below I've created a snippet of code, which you can copy and use to create the validation curves. This is essentially a function, but I've refrained from creating a function so you can easily change it around.\n",
    "\n",
    "To use it, we need to define four things:\n",
    "\n",
    "- A modelling pipeline, e.g. a `Pipeline` with `PolynomialFeatures`, `StandardScaling` then `Lasso`\n",
    "- A scoring method, e.g. `neg_mean_squared_error` or `accuracy`, see [this list for potential candidates](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n",
    "- A hyperparameter range, e.g. `np.logspace(-4, 4, 10)`\n",
    "- The name of the modelling step and the hyperparameter name, e.g. `lasso__alpha`\n",
    "\n",
    "\n",
    "Also make sure that your development data is called `X_dev` and `y_dev`.\n",
    "\n",
    "Note that you can change the scale (normal vs log) by changing `logx` to `False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling pipeline we want to use\n",
    "pipeline = # FILL IN\n",
    "\n",
    "# The measure we want to evaluate or model against\n",
    "score_type = # FILL IN\n",
    "\n",
    "# A range of hyperparameter values we want to examine\n",
    "param_range = # FILL IN\n",
    "\n",
    "# The name of the step in the pipeline and the name of the hyperparameter\n",
    "param_name = # FILL IN\n",
    "\n",
    "# Calculate train and test scores using 5 fold cross validation\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator = pipeline,\n",
    "                     X = X_dev,\n",
    "                     y = y_dev,\n",
    "                     param_name = param_name,\n",
    "                     param_range = param_range,       \n",
    "                     cv = 5)\n",
    "\n",
    "# Convert train and test scores into a DataFrame\n",
    "score_df = pd.DataFrame({'Train':train_scores.mean(axis=1),\n",
    "                          'Validation':test_scores.mean(axis=1),\n",
    "                          param_name:param_range})\n",
    "\n",
    "# Plot the scores as a function of hyperparameter\n",
    "f, ax = plt.subplots()\n",
    "score_df.set_index(param_name).plot(logx=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Here I give an example with LogisticRegression, as this is the only model we are going to be examining today which only supports classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Pipeline with StandardScaler and LogisticRegression (could add PolynomialFeatures etc.)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logit', LogisticRegression())\n",
    "])\n",
    "\n",
    "# I want to evaluate the hyperparameter with accuracy\n",
    "score_type = 'accuracy'\n",
    "\n",
    "# Logarithmically spaced between 10^-4 and 10^4\n",
    "param_range = np.logspace(-2, 2, 20)\n",
    "\n",
    "# Model step is called 'logit', hyperparameter is called 'C'\n",
    "param_name = 'logit__C' # Remember two underscores\n",
    "\n",
    "# Calculate train and test scores using 5 fold cross validation\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator = pipeline,\n",
    "                     X = X_dev,\n",
    "                     y = y_dev,\n",
    "                     scoring = score_type,\n",
    "                     param_name = param_name,\n",
    "                     param_range = param_range,       \n",
    "                     cv = 5)\n",
    "\n",
    "# Convert train and test scores into a DataFrame\n",
    "score_df = pd.DataFrame({'Train':train_scores.mean(axis=1),\n",
    "                          'Validation':test_scores.mean(axis=1),\n",
    "                          param_name:param_range})\n",
    "                \n",
    "\n",
    "# Plot the scores as a function of hyperparameter\n",
    "f, ax = plt.subplots()\n",
    "score_df.set_index(param_name).plot(logx=True, ax=ax)\n",
    "ax.set_ylabel(score_type)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we find that lower values of `C` corresponds to higher regularization, which causes the model to underfit on both the training and test data. For higher values of `C` the model starts to overfit, where we see a gap between the train and validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.3**\n",
    "> \n",
    "> Having now examined how the logistic regression, we want to see how it performs on the test data. Create a pipeline with the best hyperparameter found before, fit on the development data and calculate the accuracy on the test data.\n",
    "> \n",
    ">> *Hints:*  \n",
    ">>\n",
    ">> Try importing `accuracy_score` from `sklearn.metrics`\n",
    ">>\n",
    ">> `best_param = score_df.iloc[score_df['Validation'].idxmax()][param_name]` gets the hyperparameter for the highest validation score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.4**\n",
    "> \n",
    "> Plot the confusion matrix using the pipeline from last exercise using the test set. Has the model learnt anything useful?\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Try importing `ConfusionMatrixDisplay` from `sklearn.metrics`\n",
    ">>\n",
    ">> If this fails, there also exists a deprecated function `sklearn.metrics.plot_confusion_matrix`, which is available in previous versions of `sklearn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.5**\n",
    "> \n",
    "> If you're using the dataset I gave you, you might have noticed that the class distribution is not completely equal, which can be seen both using summary statistics and in the confusion matrix. In this setting, a baseline model becomes even more important, as a model which guesses the majority class all the time might perform quite well if the data is imbalanced enough.\n",
    ">\n",
    "> Create a pipeline with a baseline model that always guesses the majority class\n",
    "> \n",
    ">> *Hints:* \n",
    ">> \n",
    ">> try importing `DummyClassifier` from `sklearn.dummy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1.6 (OPTIONAL)**\n",
    "> \n",
    "> What would the confusion matrix for this dummy classifier look like? Try plotting it: Was your intuition correct?\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Try importing `ConfusionMatrixDisplay` from `sklearn.metrics`\n",
    ">>\n",
    ">> If this fails, there also exists a deprecated function `sklearn.metrics.plot_confusion_matrix`, which is available in previous versions of `sklearn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Having now examined a logistic regression, baseline models and the confusion matrix, we turn to the more exotic models you were introduced to today, starting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.1**\n",
    "> \n",
    "> What does the `max_depth` parameter in a decision tree do? Does the model overfit more or less if you increase this value?\n",
    "> >\n",
    "> Create a validation plot with values of `max_depth`. Use the values `np.unique(np.logspace(0, 4, 10).astype(int))` which returns integers which are evenly spaced on a log scale. Why should they be converted to integers? \n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Try importing `DecisionTreeClassifier` or `DecisionTreeRegressor` from `sklearn.tree`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.2**\n",
    "> \n",
    "> What does the `min_samples_split` parameter in a decision tree do? Does the model overfit more or less if you increase this value?\n",
    "> \n",
    "> Create a validation plot with values of `min_samples_split`. Use the values `np.arange(0.05, 1.05, 0.05)` which returns fractions from 0.05 to 1, spaced 0.05 apart. \n",
    "> \n",
    "> What do these fractions mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.3**\n",
    "> \n",
    "> What does the `min_samples_leaf` parameter in a decision tree do? Does the model overfit more or less if you increase this value?\n",
    "> \n",
    "> Create a validation plot with values of `min_samples_split`. Use the values `np.arange(2, 50, 2)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.4**\n",
    "> \n",
    "> To find the best hyperparamter values, implement a randomized search (`RandomizedSearchCV`) using the previous hyperparameter ranges. Use `n_iter = 25`. If your model takes too long to run, you can change this parameter -- should you increase it or lower it to reduce running time?\n",
    "> What are the best hyperparameters?\n",
    ">>\n",
    ">> *Hints*:\n",
    ">> \n",
    ">>  Look at exercise 2.6 from exercise session 3 for inspiration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.5**\n",
    "> \n",
    "> Calculate the accuracy of your model with the best hyperparameters. Is it better than the baseline?\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> If you are using regression data, you can compare to a baseline with `DummyRegressor` from `sklearn.dummy`\n",
    ">>\n",
    ">> Feel free to plot the confusion matrix as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As covered in the lectures, there exists two overarching ensemble methods, bagging and boosting.\n",
    "\n",
    "- For bagging, we use bootstrap aggregation to train many models, averaging their predictions afterwards.\n",
    "- For boosting, we sequentially train models, optimizing them to aid each other in the prediction task.\n",
    "\n",
    "As examples of these two ensemble methods, we covered Random Forests, a bagging algorithm, and AdaBoost, a boosting algorithm, which we will cover in the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.1**\n",
    "> \n",
    ">  The Random Forest has all the same hyperparameters as the decision tree, but also a few new. For each point below, explain what the hype parameter pertaining to `sklearn.ensemble.RandomForestClassifier` controls, and how setting it either too low or too high (or True/False) might hurt model performance:\n",
    "1. `n_estimators`\n",
    "2. `max_depth`\n",
    "3. `max_features`\n",
    "4. `bootstrap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.2**\n",
    "> \n",
    "> For `n_estimators > 1`, how should one set the hyperparameters `max_features` and `bootstrap` so that all the trees in the ensemble end up identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.3**\n",
    "> \n",
    "> Create a validation plot with values of `n_estimators`. Use the values `np.unique(np.logspace(0, 3, 25).astype(int))`. How does it influence the train and validation scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.4**\n",
    "> \n",
    "> What does the `max_features` parameter in a Random Forest do? Does the model overfit more or less if you increase this value?\n",
    "> \n",
    "> Create a validation plot with values of `max_features`. Use the values `np.arange(0.1, 1.01, 0.1)`. Does it influence the train and validation scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.5 (OPTIONAL)**\n",
    "> \n",
    "> To find the best hyperparamter values, implement a randomized search (`RandomizedSearchCV`) using the previous hyperparameter ranges, including the decision tree section. Use `n_iter = 10`. If your model takes too long to run, you can change this parameter -- should you increase it or lower it to reduce running time?\n",
    "> What are the best hyperparameters? How does the model perform on the test set?\n",
    ">>\n",
    ">> *Hints*:\n",
    ">> \n",
    ">>  Look at exercise 2.6 from exercise session 3 for inspiration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost (Boosting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.1**\n",
    "> \n",
    "> What does the `n_estimators` parameter in a AdaBoost do? Does the model overfit more or less if you increase this value?\n",
    "> \n",
    "> Create a validation plot with values of `n_estimators`. Use the values `[int(x) for x in np.linspace(start = 1, stop = 500, num = 10)]` \n",
    ">>\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Try importing `AdaBoostClassifier` from `sklearn.ensemble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.2**\n",
    "> \n",
    "> As AdaBoost is a boosting algorithm, it is designed to use weak learners. What does this imply for the hyperparameter space you should search over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.3 (OPTIONAL)**\n",
    "> \n",
    "> Iterate over the hyperparameter grid given below using `RandomizedSearchCV` with `n_iter = 10`. Are there any new hyperparameters that you haven't seen before? Consider whether you are getting any corner solutions? What does this imply for your hyperparameter search?\n",
    "> \n",
    ">\n",
    "> Note how I specify hyperparameters in the decision tree using `__` twice, first to access `base_estimator` and then the base estimators hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('adaboost', AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid= [{\n",
    "    'adaboost__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 4)],\n",
    "    'adaboost__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'adaboost__base_estimator__max_depth': [1, 5, 9],\n",
    "    'adaboost__base_estimator__min_samples_split': [2, 5, 9],\n",
    "    'adaboost__base_estimator__min_samples_leaf': [1, 3, 5],\n",
    "    'adaboost__base_estimator__max_leaf_nodes': [2, 5, 9],\n",
    "    } ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "As a small aside, there exists a subset of boosting models called Gradient Boosting models. These models are very powerful, and you should be aware that they exist. In essence, instead of changing weights of samples, they are trained to minimize the residual. \n",
    "\n",
    "One example from `sklearn` is `GradientBoostingClassifier`, see documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and `HistGradientBoostingClassifier`, see documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier), which also have `Regressor` counterparts. \n",
    "\n",
    "The perhaps most popular is `XGBoost`. It is not implemented in `sklearn`, but it uses the same interface, so the process is exactly the same with `fit` and `predict`. See the documentation [here](https://xgboost.readthedocs.io/en/stable/python/python_intro.html#). The source is Chen, T., & Guestrin, C. (2016, August). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794). \n",
    "\n",
    "Other boosting algorithms are `LightGBM` for efficient training and `CatBoost` for many categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "## A visual inspection of neural networks\n",
    "Instead of diving into code, it's more important that our intuition about what neural networks are doing is as good as possible. The best (and most fun) way to do that is to play around and with things a bit, so go familiarize yourself with the [Tensorflow Playground](https://playground.tensorflow.org/), slide some knobs and pull some levers. The example in the lecture uses the same idea for demonstrating the intuition of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5.1** \n",
    ">\n",
    "> Using the dataset with the two point clouds, create the minimal neural network that separates the clusters. You can share your answer with a link (the URL on playground.tensorflow.org changes as you update the network, so at any time you can use the link to show others what you have created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5.2** \n",
    "> \n",
    "> Using the dataset with the two circular clusters, one inner and one outer. Create the minimal neural network that separates the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5.3 (OPTIONAL)** \n",
    "> \n",
    "> See if you can create a network that performs well on the the dataset with the intertwined spirals. Can you do it with only $x_1$ and $x_2$?\n",
    ">> *Hints*: \n",
    ">>\n",
    ">> Try experimenting with depth of the network, regularization and possibly the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having now slid some knobs and pulled some levers to get some intuition for how the neural networks operate, we turn to the Multilayer Perceptron in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 5.4 (OPTIONAL)**\n",
    "> \n",
    "> Try to create a neural network which performs better than the best model on the test data. You may want to consider looking at different strengths of regularization (`alpha`, perhaps using `np.logspace`) and different amounts of hidden layers and hidden neurons. At this point in time, a just semi-exhaustive search of hyperparameters becomes computationally infeasible, and machine learning turns to art. \n",
    "> \n",
    "> *Note:* It is not given that a neural network performs best for the given problem, and even if the model exists, it may be hard to find the right architecture. I have not succeeded.\n",
    ">\n",
    ">> *Hints:* \n",
    ">>\n",
    ">> It may be time-consuming to do k fold cross validation. Splitting your development data into a train and validation set a single time is also a possibility. Only rule is that you don't use the test data for model selection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vive_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "50f1b660881291c5d31ad4d40c48915b3ef9364cc881d538585b11a7eb841304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
