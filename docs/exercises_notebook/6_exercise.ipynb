{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 6: Explainability\n",
    "\n",
    "In this exercise set, we will be looking at explainability.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset we will be looking at this time is a dataset regarding the determinants of wages from the 1985 Current Population Survey (Berndt, ER. The Practice of Econometrics. 1991. NY: Addison-Wesley), which is open source and available at [OpenML](https://www.openml.org/search?type=data&status=active&id=534). As last time, you're welcome to use a dataset of your own.\n",
    "\n",
    "**Load data**\n",
    "\n",
    "Here we load our input data into a `DataFrame` called `X` and our target data into a `Series` called `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T12:18:56.210482Z",
     "start_time": "2020-02-25T12:18:55.219063Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Get wage data\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "survey = fetch_openml(data_id=534, as_frame=True)\n",
    "\n",
    "X = survey.data[survey.feature_names]\n",
    "\n",
    "y = survey.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe the data using both the documentation which came with the data, but also by computing summary statistics for the input data and target data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**:   \n",
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**:   \n",
      "\n",
      "Determinants of Wages from the 1985 Current Population Survey\n",
      "\n",
      "Summary:\n",
      "The Current Population Survey (CPS) is used to supplement census information between census years. These data consist of a random sample of 534 persons from the CPS, with information on wages and other characteristics of the workers, including sex, number of years of education, years of work experience, occupational status, region of residence and union membership. We wish to determine (i) whether wages are related to these characteristics and (ii) whether there is a gender gap in wages.\n",
      "Based on residual plots, wages were log-transformed to stabilize the variance. Age and work experience were almost perfectly correlated (r=.98). Multiple regression of log wages against sex, age, years of education, work experience, union membership, southern residence, and occupational status showed that these covariates were related to wages (pooled F test, p < .0001). The effect of age was not significant after controlling for experience. Standardized residual plots showed no patterns, except for one large outlier with lower wages than expected. This was a male, with 22 years of experience and 12 years of education, in a management position, who lived in the north and was not a union member. Removing this person from the analysis did not substantially change the results, so that the final model included the entire sample.\n",
      "Adjusting for all other variables in the model, females earned 81% (75%, 88%) the wages of males (p < .0001). Wages increased 41% (28%, 56%) for every 5 additional years of education (p < .0001). They increased by 11% (7%, 14%) for every additional 10 years of experience (p < .0001). Union members were paid 23% (12%, 36%) more than non-union members (p < .0001). Northerns were paid 11% (2%, 20%) more than southerns (p =.016). Management and professional positions were paid most, and service and clerical positions were paid least (pooled F-test, p < .0001). Overall variance explained was R2 = .35.\n",
      "In summary, many factors describe the variations in wages: occupational status, years of experience, years of education, sex, union membership and region of residence. However, despite adjustment for all factors that were available, there still appeared to be a gender gap in wages. There is no readily available explanation for this gender gap.\n",
      "\n",
      "Authorization: Public Domain\n",
      "\n",
      "Reference: Berndt, ER. The Practice of Econometrics. 1991. NY: Addison-Wesley.\n",
      "\n",
      "Description:  The datafile contains 534 observations on 11 variables sampled from the Current Population Survey of 1985.  This data set demonstrates multiple regression, confounding, transformations, multicollinearity, categorical variables, ANOVA, pooled tests of significance, interactions and model building strategies.\n",
      "\n",
      "Variable names in order from left to right:\n",
      "EDUCATION: Number of years of education.\n",
      "SOUTH: Indicator variable for Southern Region (1=Person lives in \t\tSouth, 0=Person lives elsewhere).\n",
      "SEX: Indicator variable for sex (1=Female, 0=Male).\n",
      "EXPERIENCE: Number of years of work experience.\n",
      "UNION: Indicator variable for union membership (1=Union member, \t\t0=Not union member).\n",
      "WAGE: Wage (dollars per hour).\n",
      "AGE: Age (years).\n",
      "RACE: Race (1=Other, 2=Hispanic, 3=White).\n",
      "OCCUPATION: Occupational category (1=Management, \t\t2=Sales, 3=Clerical, 4=Service, 5=Professional, 6=Other).\n",
      "SECTOR: Sector (0=Other, 1=Manufacturing, 2=Construction).\n",
      "MARR: Marital Status (0=Unmarried,  1=Married)\n",
      "\n",
      "\n",
      "Therese Stukel\n",
      "Dartmouth Hitchcock Medical Center\n",
      "One Medical Center Dr.\n",
      "Lebanon, NH 03756\n",
      "e-mail: stukel@dartmouth.edu\n",
      "\n",
      "\n",
      "Information about the dataset\n",
      "CLASSTYPE: numeric\n",
      "CLASSINDEX: none specific\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "print(survey.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>SOUTH</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EXPERIENCE</th>\n",
       "      <th>UNION</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>OCCUPATION</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>MARR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>534.000000</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>not_member</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Married</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>378</td>\n",
       "      <td>289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440</td>\n",
       "      <td>156</td>\n",
       "      <td>411</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.018727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.822097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.615373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.379710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.726573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         EDUCATION SOUTH   SEX  EXPERIENCE       UNION         AGE   RACE  \\\n",
       "count   534.000000   534   534  534.000000         534  534.000000    534   \n",
       "unique         NaN     2     2         NaN           2         NaN      3   \n",
       "top            NaN    no  male         NaN  not_member         NaN  White   \n",
       "freq           NaN   378   289         NaN         438         NaN    440   \n",
       "mean     13.018727   NaN   NaN   17.822097         NaN   36.833333    NaN   \n",
       "std       2.615373   NaN   NaN   12.379710         NaN   11.726573    NaN   \n",
       "min       2.000000   NaN   NaN    0.000000         NaN   18.000000    NaN   \n",
       "25%      12.000000   NaN   NaN    8.000000         NaN   28.000000    NaN   \n",
       "50%      12.000000   NaN   NaN   15.000000         NaN   35.000000    NaN   \n",
       "75%      15.000000   NaN   NaN   26.000000         NaN   44.000000    NaN   \n",
       "max      18.000000   NaN   NaN   55.000000         NaN   64.000000    NaN   \n",
       "\n",
       "       OCCUPATION SECTOR     MARR  \n",
       "count         534    534      534  \n",
       "unique          6      3        2  \n",
       "top         Other  Other  Married  \n",
       "freq          156    411      350  \n",
       "mean          NaN    NaN      NaN  \n",
       "std           NaN    NaN      NaN  \n",
       "min           NaN    NaN      NaN  \n",
       "25%           NaN    NaN      NaN  \n",
       "50%           NaN    NaN      NaN  \n",
       "75%           NaN    NaN      NaN  \n",
       "max           NaN    NaN      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    534.000000\n",
       "mean       9.024064\n",
       "std        5.139097\n",
       "min        1.000000\n",
       "25%        5.250000\n",
       "50%        7.780000\n",
       "75%       11.250000\n",
       "max       44.500000\n",
       "Name: WAGE, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise set, we will not perform hyperparameteroptimization, as this simplifies the code.\n",
    "\n",
    "However, some methods can only explain the training data, whereas some can explain both the training and test data. To examine this, we split up the sample.\n",
    "\n",
    "> **Exercise 1.1**\n",
    "> \n",
    "> Fill in the missing code such that we split the data into 80% train and 20% test.\n",
    ">\n",
    ">> *Hints:*\n",
    ">> \n",
    ">> We have previously looked at datasplitting. Try looking at last sessions exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import FILL IN\n",
    "\n",
    "X_train, X_test, y_train, y_test = FILL IN(FILL IN, random_state=73)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "In this section of the exercises, we will be looking at (regularized) linear regression.\n",
    "\n",
    "We will be looking at different types of plots, and how regularizing linear regression with LASSO can create sparse models.\n",
    "\n",
    "First we will use some of `sklearn`'s built-in functionality for easily handling categorical variables. \n",
    "\n",
    "> **Exercise 2.1**\n",
    "> \n",
    "> What does this code do? Fill in the missing comments.\n",
    ">\n",
    ">> *Hints:*\n",
    ">> \n",
    ">> We have not used this functionality before. \n",
    ">>\n",
    ">> If you're in doubt, try looking up the documentation.\n",
    ">>\n",
    ">> Think about how numerical and categorical variables should be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "#\n",
    "categorical_columns = [\"RACE\", \"OCCUPATION\", \"SECTOR\", \"MARR\", \"UNION\", \"SEX\", \"SOUTH\"]\n",
    "numerical_columns = [\"EDUCATION\", \"EXPERIENCE\", \"AGE\"]\n",
    "\n",
    "#\n",
    "categorical_encoder = OneHotEncoder()\n",
    "\n",
    "#\n",
    "col_transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_encoder, categorical_columns)\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out with, we only introduce a slight amount of regularization. \n",
    "\n",
    "> **Exercise 2.2**\n",
    "> \n",
    "> Fill in the missing code and comments to implement a LASSO regression with $\\alpha=0.0001$.\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> We created a transformer for categorical columns in the previous exercise.\n",
    ">>\n",
    ">> We have looked at this in previous sessions. Try looking at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FILL IN \n",
    "from sklearn.linear_model import FILL IN \n",
    "from sklearn.preprocessing import FILL IN \n",
    "\n",
    "#\n",
    "lasso_model = Pipeline([\n",
    "    ('preprocessor',FILL IN),\n",
    "    ('scaler', FILL IN),\n",
    "    ('regr', FILL IN)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "#\n",
    "feature_names = lasso_model[:-1].get_feature_names_out()\n",
    "\n",
    "#\n",
    "lasso_coefs = pd.DataFrame(\n",
    "    lasso_model[-1].coef_,\n",
    "    columns=[\"Coefficients\"],\n",
    "    index=feature_names,\n",
    ")\n",
    "\n",
    "lasso_coefs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from session 4, LASSO creates sparse models due to the corner solutions stemming from L1 normalization.\n",
    "\n",
    "> **Exercise 2.3**\n",
    "> \n",
    "> Looking at the coefficients from exercise 2.2, which coefficients are equal to zero?\n",
    "> Why is the second line of code necessary to get the answer we intuitively agree with?\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Think about numerical accuracy and computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Equal to zero')\n",
    "print(lasso_coefs.loc[lasso_coefs.Coefficients == 0])\n",
    "print()\n",
    "print('Close to zero')\n",
    "print(lasso_coefs.loc[np.isclose(lasso_coefs.Coefficients, 0)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.4**\n",
    "> \n",
    "> The following code creates a weight plot for the coefficients. How should these be interpreted? Fill in the missing comments.\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> On what scale are the features measured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "#\n",
    "sns.pointplot(data=lasso_coefs.reset_index(), x='Coefficients', y='index', join=False, ax=ax)\n",
    "#\n",
    "plt.title(\"Lasso model, low regularization\")\n",
    "#\n",
    "plt.axvline(x=0, color=\"grey\", linestyle='--')\n",
    "#\n",
    "plt.xlabel(\"Coefficient values\")\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.5**\n",
    "> \n",
    "> The following code creates an effect plot for the model. How should these be interpreted? Fill in the missing comments.\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> On what scale are the effects measured?\n",
    ">>\n",
    ">> What does `[:-1]` and `[-1]` select from the pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "lasso_contributions = lasso_model[:-1].transform(X_train) * lasso_model[-1].coef_\n",
    "df_lasso_contributions = pd.DataFrame(lasso_contributions, columns=feature_names)\n",
    "\n",
    "#\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "sns.boxplot(data=df_lasso_contributions, orient='h', ax=ax)\n",
    "plt.axvline(x=0, color=\"grey\", linestyle='--')\n",
    "plt.xlabel('Prediction contribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.6**\n",
    "> \n",
    "> The following code creates an effect plot for the model with an individual plotted as well. In what situations would a plot like this be useful? Fill in the missing comments.\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Who's the receiver of the explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "sns.boxplot(data=df_lasso_contributions, ax=ax)\n",
    "# \n",
    "sns.scatterplot(data=df_lasso_contributions.iloc[0].T, s=200, color=\"red\", marker=\"x\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.axhline(y=0, color=\"grey\", linestyle='--')\n",
    "plt.ylabel('Prediction contribution')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2.7**\n",
    "> \n",
    "> Try running the code with other values of $\\alpha$. Does the model become easier to interpret? Are the same coefficients always high/low? \n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Multicollinearity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "In this section, we will be looking at how to plot decision trees as flowcharts. \n",
    "\n",
    "> **Exercise 3.1**\n",
    "> \n",
    "> Fill in the missing code and comments to create a plot for a decision tree with max depth 1.\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> We've covered this in previous sessions. Try looking at our previous materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, FILL IN \n",
    "\n",
    "#\n",
    "decision_model = Pipeline([\n",
    "    ('preprocessor', col_transformer),\n",
    "    ('regr', FILL IN)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#\n",
    "decision_feature_names = decision_model[0].get_feature_names_out()\n",
    "\n",
    "#\n",
    "decision_model.fit(X_train, y_train)\n",
    "\n",
    "#\n",
    "plt.figure(figsize=(10,10)) \n",
    "plot_tree(decision_model[-1], feature_names = decision_feature_names, impurity=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 3.2**\n",
    "> \n",
    "> Repeat the following exercise with max depth 2 and 5. Is the model still explainable?\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Are you explaining a single prediction or the full model?\n",
    ">>\n",
    ">> To increase legibility, try changing the `figsize` parameter, e.g. `(50,10)` for the larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model agnostic methods\n",
    "\n",
    "In this section of the exercises, we will cover model agnostic methods, such as permutation feature importance, partial dependence plots and SHAP values.\n",
    "\n",
    "We first create a random forest model, which is not intrinsically interpretable. This is the model we want to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = Pipeline([\n",
    "    ('preprocessor', col_transformer),\n",
    "    ('regr', RandomForestRegressor())\n",
    "    ]\n",
    ")\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "feature_names = forest_model[:-1].get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation feature importance\n",
    "\n",
    "> **Exercise 4.1**\n",
    "> \n",
    "> Fill in the missing comments to create a feature importance plot. How does one interpret these plots? What's the difference between the two plots?\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> How are permutation feature importances calculated, and what is measured? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#\n",
    "result = permutation_importance(\n",
    "    forest_model, X_train, y_train, n_repeats=10, random_state=73, n_jobs=2, scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "#\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_test.columns[sorted_importances_idx],\n",
    ")\n",
    "\n",
    "#\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "result = permutation_importance(\n",
    "    forest_model, X_test, y_test, n_repeats=10, random_state=73, n_jobs=2, scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "#\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X_test.columns[sorted_importances_idx],\n",
    ")\n",
    "\n",
    "#\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial dependencies\n",
    "\n",
    "`sklearn.inspection` has built-in functionality for creating plots from partial dependencies. An example including both partial dependence plots and individual conditional expectations can be seen on [this page](https://scikit-learn.org/stable/modules/partial_dependence.html).\n",
    "\n",
    "> **Exercise 4.2**\n",
    "> \n",
    "> Create two one-way partial dependence plots for `EDUCATION` and `AGE`, as well as a two-way partial dependence plot in a single figure. Use the training data as background data.\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> This corresponds to part one of the first plot on the webpage. \n",
    ">>\n",
    ">> The code uses feature column indices to note what features to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Index to feature\n",
    "for idx, col in enumerate(X_train.columns):\n",
    "    print(idx, col)\n",
    "\n",
    "# Alternatively, use dictionary\n",
    "mapper = {name:idx for idx, name in enumerate(X.columns)}\n",
    "\n",
    "# Input model, data and columns\n",
    "PartialDependenceDisplay.from_estimator(FILL IN)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of masking the heterogeneity, we can also limit ourselves to one-way plots and display the heterogeneity in the partial dependencies, creating so-called individual expectation plots.\n",
    "\n",
    "> **Exercise 4.3**\n",
    "> \n",
    "> Create an individual conditional expectation plot for both `EDUCATION` and `AGE` in a single plot. It should first be uncentered and then centered, and always include the average. Which do you prefer?\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> Change the `kind` of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP\n",
    "\n",
    "Finally, we have the game theoretically founded SHapley Additive exPlanations, which are calculated using the [SHAP](https://github.com/slundberg/shap) package. They have a lot of different examples on their page, so look to there for code! \n",
    "\n",
    "Note that all of the following plots can be created using a single line of code, which perhaps is also one of the causes for it's popularity. \n",
    "\n",
    "\n",
    "> **Exercise 4.4**\n",
    "> \n",
    "> Fill in the missing comments, and explain to yourself what each of the inputs to the `shap.TreeExplainer` does.\n",
    ">\n",
    "> Does the fact that we use training data as our background data influence what observations we can calculate SHAP values for?\n",
    ">\n",
    ">> *Hints*:\n",
    ">>\n",
    ">> The `shap.TreeExplainer` only accepts arrays as data input.\n",
    ">>\n",
    ">> What is the background data used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "#\n",
    "forest_feature_names = forest_model[:-1].get_feature_names_out()\n",
    "\n",
    "#\n",
    "X_train_as_array = forest_model[:-1].transform(X_train)\n",
    "\n",
    "#\n",
    "explainer = shap.TreeExplainer(model = forest_model[-1], data = X_train_as_array, feature_names=forest_feature_names, seed=73, feature_perturbation = 'interventional')\n",
    "\n",
    "#\n",
    "shap_values = explainer(X_train_as_array,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.5**\n",
    "> \n",
    "> Create a `force` plot of the first observation\n",
    ">\n",
    ">> *Hints*:\n",
    ">> \n",
    ">> If the package complains regarding initialization of js, try adding `matplotlib=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Exercise 4.5**\n",
    ">\n",
    "> Sometimes the force plots can get a bit messy with long column names.\n",
    "> \n",
    "> Create a `waterfall` plot of the first observation.\n",
    ">\n",
    "> Did it help with the issue? What are possible downsides?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.6**\n",
    ">\n",
    "> Although SHAP values are local explanations, they can still be aggregated for groups to gain global insights.\n",
    "> \n",
    "> Create a `bar` plot of the mean absolute SHAP value for each feature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.7**\n",
    ">\n",
    "> We can also create plots akin to partial dependence plots, examining heterogeneity for different values of a feature\n",
    "> \n",
    "> Create a `scatter` plot of the SHAP values for `EDUCATION`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4.8**\n",
    "> \n",
    "> To examine interactions between features, dependence plots can be coloured by another feature. \n",
    "> \n",
    "> Create a `scatter` plot of the SHAP values for `EDUCATION` where the colour is decided by the value of `AGE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can examine heterogeneity for multiple features at once using `beeswarm` plots.\n",
    "\n",
    "> **Exercise 4.9**\n",
    "> \n",
    "> Create a `beeswarm` plot of the SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vive_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "50f1b660881291c5d31ad4d40c48915b3ef9364cc881d538585b11a7eb841304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
