<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine learning course for VIVE – exercise</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../images/favicon.ico" rel="icon">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Machine learning course for VIVE</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">Home</a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">



<section id="exercise-set-4-supervised-learning" class="level1">
<h1>Exercise Set 4: Supervised learning</h1>
<p>In this exercise set, we will mainly be looking at different supervised learning algorithms, both tinkering around with them and seeing how the models perform for a given dataset. We will look at:</p>
<ul>
<li>Logistic regression</li>
<li>Decision tree</li>
<li>Ensemble methods
<ul>
<li>Random Forest</li>
<li>AdaBoost</li>
</ul></li>
<li>Neural network</li>
</ul>
<p>If you in general need more information about models or how to tune their hyperparameters, try looking up the documentation or googling <code>hyperparameter tuning + model_name</code></p>
<p>Throughout your career, you’ve probably worked with many problems. Some problems can easily be formulated as regression problem, whereas others are easily formulated as a classification problem.</p>
<blockquote class="blockquote">
<p><strong>Exercise 1.1</strong></p>
<p>Name three different problems which you’ve worked with where the outcome of interest was:</p>
<ul>
<li><p>Continuous (regression)</p></li>
<li><p>Categorical (classification)</p></li>
</ul>
<p>Have you encountered problems where the outcome of interest could be both continuous and categorical? Would being able to predict these outcomes of interest be valuable?</p>
</blockquote>
<p>For this session, I invite you to use a dataset of your own, as different models work best for different problems: - This can be either a regression problem or a classification problem. - Feel free to preprocess in another program and export it as a <code>csv</code> file or another format of your choosing</p>
<p>The exercises are designed with a classification problem in mind, but all exercises except the ones about confusion matrices can be exchanged for regression problems by changin from a <code>Classifier</code> to a <code>Regressor</code> model.</p>
<p>The dataset I’ve decided upon is a dataset regarding classification of high income people, namely the <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">Census Income Data Set from the UCI Machine Learning Repository</a>. I’ve reduced the amount of features and sample size, as well as done a little bit of cleaning, from the full sample to reduce computation time. All the categorical features are one-hot encoded.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppress convergencewarnings if they appear</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.exceptions <span class="im">import</span> ConvergenceWarning</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(action<span class="op">=</span><span class="st">'ignore'</span>, category<span class="op">=</span>ConvergenceWarning)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual code to load</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'adult_preprocessed.csv'</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>df.describe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p><strong>Exercise 1.2</strong></p>
<p>What column in the DataFrame is the target of interest? Subset this as a <code>Series</code> called <code>y</code>, and the rest of the columns as a <code>DataFrame</code> called <code>X</code></p>
<blockquote class="blockquote">
<p><em>Hints:</em></p>
<p><code>y = df['column_name']</code> subsets a column as a Series.</p>
<p><code>X = df.drop(columns='column_name')</code> drops a column in a dataframe</p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 1.3</strong></p>
<p>As a first step, you should split the data into a development and test set. Make a development and test split with 80% of the data in the development set. Name them <code>X_dev</code>, <code>X_test</code>, <code>y_dev</code> and <code>y_test</code></p>
<blockquote class="blockquote">
<p><em>Hints:</em></p>
<p>Try importing <code>train_test_split</code> from <code>sklearn.model_selection</code></p>
</blockquote>
</blockquote>
</section>
<section id="validation-curves" class="level1">
<h1>Validation curves</h1>
<p>Last week, you were introduced to validation curves. This is a way of getting an understanding how a single hyperparameter changes the performance of a model on both seen and unseen data. We will be using this tool throughout these exercises to probe the models and see how the hyperparameters change the performance of the model.</p>
<p>Below I’ve created a snippet of code, which you can copy and use to create the validation curves. This is essentially a function, but I’ve refrained from creating a function so you can easily change it around.</p>
<p>To use it, we need to define four things:</p>
<ul>
<li>A modelling pipeline, e.g.&nbsp;a <code>Pipeline</code> with <code>PolynomialFeatures</code>, <code>StandardScaling</code> then <code>Lasso</code></li>
<li>A scoring method, e.g.&nbsp;<code>neg_mean_squared_error</code> or <code>accuracy</code>, see <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules">this list for potential candidates</a></li>
<li>A hyperparameter range, e.g.&nbsp;<code>np.logspace(-4, 4, 10)</code></li>
<li>The name of the modelling step and the hyperparameter name, e.g.&nbsp;<code>lasso__alpha</code></li>
</ul>
<p>Also make sure that your development data is called <code>X_dev</code> and <code>y_dev</code>.</p>
<p>Note that you can change the scale (normal vs log) by changing <code>logx</code> to <code>False</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> validation_curve</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Modelling pipeline we want to use</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> <span class="co"># FILL IN</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The measure we want to evaluate or model against</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>score_type <span class="op">=</span> <span class="co"># FILL IN</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># A range of hyperparameter values we want to examine</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>param_range <span class="op">=</span> <span class="co"># FILL IN</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The name of the step in the pipeline and the name of the hyperparameter</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>param_name <span class="op">=</span> <span class="co"># FILL IN</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate train and test scores using 5 fold cross validation</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>train_scores, test_scores <span class="op">=</span> <span class="op">\</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    validation_curve(estimator <span class="op">=</span> pipeline,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>                     X <span class="op">=</span> X_dev,</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>                     y <span class="op">=</span> y_dev,</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>                     param_name <span class="op">=</span> param_name,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>                     param_range <span class="op">=</span> param_range,       </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                     cv <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert train and test scores into a DataFrame</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>score_df <span class="op">=</span> pd.DataFrame({<span class="st">'Train'</span>:train_scores.mean(axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                          <span class="st">'Validation'</span>:test_scores.mean(axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>                          param_name:param_range})</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the scores as a function of hyperparameter</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>score_df.set_index(param_name).plot(logx<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>ax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="logistic-regression" class="level1">
<h1>Logistic Regression</h1>
<p>Here I give an example with LogisticRegression, as this is the only model we are going to be examining today which only supports classification.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> validation_curve</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Additional imports</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline with StandardScaler and LogisticRegression (could add PolynomialFeatures etc.)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'scaler'</span>, StandardScaler()),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'logit'</span>, LogisticRegression())</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># I want to evaluate the hyperparameter with accuracy</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>score_type <span class="op">=</span> <span class="st">'accuracy'</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Logarithmically spaced between 10^-4 and 10^4</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>param_range <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">20</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Model step is called 'logit', hyperparameter is called 'C'</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>param_name <span class="op">=</span> <span class="st">'logit__C'</span> <span class="co"># Remember two underscores</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate train and test scores using 5 fold cross validation</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>train_scores, test_scores <span class="op">=</span> <span class="op">\</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    validation_curve(estimator <span class="op">=</span> pipeline,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>                     X <span class="op">=</span> X_dev,</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>                     y <span class="op">=</span> y_dev,</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>                     scoring <span class="op">=</span> score_type,</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>                     param_name <span class="op">=</span> param_name,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                     param_range <span class="op">=</span> param_range,       </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                     cv <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert train and test scores into a DataFrame</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>score_df <span class="op">=</span> pd.DataFrame({<span class="st">'Train'</span>:train_scores.mean(axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>                          <span class="st">'Validation'</span>:test_scores.mean(axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>                          param_name:param_range})</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the scores as a function of hyperparameter</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>score_df.set_index(param_name).plot(logx<span class="op">=</span><span class="va">True</span>, ax<span class="op">=</span>ax)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(score_type)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As expected, we find that lower values of <code>C</code> corresponds to higher regularization, which causes the model to underfit on both the training and test data. For higher values of <code>C</code> the model starts to overfit, where we see a gap between the train and validation scores.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>score_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p><strong>Exercise 1.3</strong></p>
<p>Having now examined how the logistic regression, we want to see how it performs on the test data. Create a pipeline with the best hyperparameter found before, fit on the development data and calculate the accuracy on the test data.</p>
<blockquote class="blockquote">
<p><em>Hints:</em></p>
<p>Try importing <code>accuracy_score</code> from <code>sklearn.metrics</code></p>
<p><code>best_param = score_df.iloc[score_df['Validation'].idxmax()][param_name]</code> gets the hyperparameter for the highest validation score</p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 1.4</strong></p>
<p>Plot the confusion matrix using the pipeline from last exercise using the test set. Has the model learnt anything useful? &gt; <em>Hints</em>: &gt; &gt; Try importing <code>ConfusionMatrixDisplay</code> from <code>sklearn.metrics</code> &gt; &gt; If this fails, there also exists a deprecated function <code>sklearn.metrics.plot_confusion_matrix</code>, which is available in previous versions of <code>sklearn</code></p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 1.5</strong></p>
<p>If you’re using the dataset I gave you, you might have noticed that the class distribution is not completely equal, which can be seen both using summary statistics and in the confusion matrix. In this setting, a baseline model becomes even more important, as a model which guesses the majority class all the time might perform quite well if the data is imbalanced enough.</p>
<p>Create a pipeline with a baseline model that always guesses the majority class</p>
<blockquote class="blockquote">
<p><em>Hints:</em></p>
<p>try importing <code>DummyClassifier</code> from <code>sklearn.dummy</code></p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 1.6 (OPTIONAL)</strong></p>
<p>What would the confusion matrix for this dummy classifier look like? Try plotting it: Was your intuition correct?</p>
<blockquote class="blockquote">
<p><em>Hints</em>:</p>
<p>Try importing <code>ConfusionMatrixDisplay</code> from <code>sklearn.metrics</code></p>
<p>If this fails, there also exists a deprecated function <code>sklearn.metrics.plot_confusion_matrix</code>, which is available in previous versions of <code>sklearn</code></p>
</blockquote>
</blockquote>
</section>
<section id="decision-tree" class="level1">
<h1>Decision Tree</h1>
<p>Having now examined a logistic regression, baseline models and the confusion matrix, we turn to the more exotic models you were introduced to today, starting with the decision tree.</p>
<blockquote class="blockquote">
<p><strong>Exercise 2.1</strong></p>
<p>What does the <code>max_depth</code> parameter in a decision tree do? Does the model overfit more or less if you increase this value? &gt; Create a validation plot with values of <code>max_depth</code>. Use the values <code>np.unique(np.logspace(0, 4, 10).astype(int))</code> which returns integers which are evenly spaced on a log scale. Why should they be converted to integers?</p>
<blockquote class="blockquote">
<p><em>Hints</em>:</p>
<p>Try importing <code>DecisionTreeClassifier</code> or <code>DecisionTreeRegressor</code> from <code>sklearn.tree</code></p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 2.2</strong></p>
<p>What does the <code>min_samples_split</code> parameter in a decision tree do? Does the model overfit more or less if you increase this value?</p>
<p>Create a validation plot with values of <code>min_samples_split</code>. Use the values <code>np.arange(0.05, 1.05, 0.05)</code> which returns fractions from 0.05 to 1, spaced 0.05 apart.</p>
<p>What do these fractions mean?</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 2.3</strong></p>
<p>What does the <code>min_samples_leaf</code> parameter in a decision tree do? Does the model overfit more or less if you increase this value?</p>
<p>Create a validation plot with values of <code>min_samples_split</code>. Use the values <code>np.arange(2, 50, 2)</code></p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 2.4</strong></p>
<p>To find the best hyperparamter values, implement a randomized search (<code>RandomizedSearchCV</code>) using the previous hyperparameter ranges. Use <code>n_iter = 25</code>. If your model takes too long to run, you can change this parameter – should you increase it or lower it to reduce running time? What are the best hyperparameters? &gt; &gt; <em>Hints</em>: &gt; &gt; Look at exercise 2.6 from exercise session 3 for inspiration</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 2.5</strong></p>
<p>Calculate the accuracy of your model with the best hyperparameters. Is it better than the baseline? &gt; <em>Hints</em>: &gt; &gt; If you are using regression data, you can compare to a baseline with <code>DummyRegressor</code> from <code>sklearn.dummy</code> &gt; &gt; Feel free to plot the confusion matrix as well</p>
</blockquote>
</section>
<section id="ensemble-model" class="level1">
<h1>Ensemble Model</h1>
<p>As covered in the lectures, there exists two overarching ensemble methods, bagging and boosting.</p>
<ul>
<li>For bagging, we use bootstrap aggregation to train many models, averaging their predictions afterwards.</li>
<li>For boosting, we sequentially train models, optimizing them to aid each other in the prediction task.</li>
</ul>
<p>As examples of these two ensemble methods, we covered Random Forests, a bagging algorithm, and AdaBoost, a boosting algorithm, which we will cover in the next two sections.</p>
<section id="random-forest-bagging" class="level2">
<h2 class="anchored" data-anchor-id="random-forest-bagging">Random Forest (Bagging)</h2>
<blockquote class="blockquote">
<p><strong>Exercise 3.1</strong></p>
<p>The Random Forest has all the same hyperparameters as the decision tree, but also a few new. For each point below, explain what the hype parameter pertaining to <code>sklearn.ensemble.RandomForestClassifier</code> controls, and how setting it either too low or too high (or True/False) might hurt model performance: 1. <code>n_estimators</code> 2. <code>max_depth</code> 3. <code>max_features</code> 4. <code>bootstrap</code></p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 3.2</strong></p>
<p>For <code>n_estimators &gt; 1</code>, how should one set the hyperparameters <code>max_features</code> and <code>bootstrap</code> so that all the trees in the ensemble end up identical?</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 3.3</strong></p>
<p>Create a validation plot with values of <code>n_estimators</code>. Use the values <code>np.unique(np.logspace(0, 3, 25).astype(int))</code>. How does it influence the train and validation scores?</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 3.4</strong></p>
<p>What does the <code>max_features</code> parameter in a Random Forest do? Does the model overfit more or less if you increase this value?</p>
<p>Create a validation plot with values of <code>max_features</code>. Use the values <code>np.arange(0.1, 1.01, 0.1)</code>. Does it influence the train and validation scores?</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 3.5 (OPTIONAL)</strong></p>
<p>To find the best hyperparamter values, implement a randomized search (<code>RandomizedSearchCV</code>) using the previous hyperparameter ranges, including the decision tree section. Use <code>n_iter = 10</code>. If your model takes too long to run, you can change this parameter – should you increase it or lower it to reduce running time? What are the best hyperparameters? How does the model perform on the test set? &gt; &gt; <em>Hints</em>: &gt; &gt; Look at exercise 2.6 from exercise session 3 for inspiration</p>
</blockquote>
</section>
<section id="adaboost-boosting" class="level2">
<h2 class="anchored" data-anchor-id="adaboost-boosting">AdaBoost (Boosting)</h2>
<blockquote class="blockquote">
<p><strong>Exercise 4.1</strong></p>
<p>What does the <code>n_estimators</code> parameter in a AdaBoost do? Does the model overfit more or less if you increase this value?</p>
<p>Create a validation plot with values of <code>n_estimators</code>. Use the values <code>[int(x) for x in np.linspace(start = 1, stop = 500, num = 10)]</code> &gt; &gt; <em>Hints</em>: &gt; &gt; Try importing <code>AdaBoostClassifier</code> from <code>sklearn.ensemble</code></p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 4.2</strong></p>
<p>As AdaBoost is a boosting algorithm, it is designed to use weak learners. What does this imply for the hyperparameter space you should search over?</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 4.3 (OPTIONAL)</strong></p>
<p>Iterate over the hyperparameter grid given below using <code>RandomizedSearchCV</code> with <code>n_iter = 10</code>. Are there any new hyperparameters that you haven’t seen before? Consider whether you are getting any corner solutions? What does this imply for your hyperparameter search?</p>
<p>Note how I specify hyperparameters in the decision tree using <code>__</code> twice, first to access <code>base_estimator</code> and then the base estimators hyperparameters.</p>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'adaboost'</span>, AdaBoostClassifier(base_estimator<span class="op">=</span>DecisionTreeClassifier()))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>param_grid<span class="op">=</span> [{</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'adaboost__n_estimators'</span>: [<span class="bu">int</span>(x) <span class="cf">for</span> x <span class="kw">in</span> np.linspace(start <span class="op">=</span> <span class="dv">200</span>, stop <span class="op">=</span> <span class="dv">2000</span>, num <span class="op">=</span> <span class="dv">4</span>)],</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'adaboost__learning_rate'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>],</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'adaboost__base_estimator__max_depth'</span>: [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>],</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'adaboost__base_estimator__min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">9</span>],</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'adaboost__base_estimator__min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'adaboost__base_estimator__max_leaf_nodes'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">9</span>],</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    } ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-boosting" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting">Gradient Boosting</h2>
<p>As a small aside, there exists a subset of boosting models called Gradient Boosting models. These models are very powerful, and you should be aware that they exist. In essence, instead of changing weights of samples, they are trained to minimize the residual.</p>
<p>One example from <code>sklearn</code> is <code>GradientBoostingClassifier</code>, see documentation <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">here</a> and <code>HistGradientBoostingClassifier</code>, see documentation <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier">here</a>, which also have <code>Regressor</code> counterparts.</p>
<p>The perhaps most popular is <code>XGBoost</code>. It is not implemented in <code>sklearn</code>, but it uses the same interface, so the process is exactly the same with <code>fit</code> and <code>predict</code>. See the documentation <a href="https://xgboost.readthedocs.io/en/stable/python/python_intro.html#">here</a>. The source is Chen, T., &amp; Guestrin, C. (2016, August). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp.&nbsp;785-794).</p>
<p>Other boosting algorithms are <code>LightGBM</code> for efficient training and <code>CatBoost</code> for many categorical features.</p>
</section>
</section>
<section id="neural-network" class="level1">
<h1>Neural network</h1>
<section id="a-visual-inspection-of-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="a-visual-inspection-of-neural-networks">A visual inspection of neural networks</h2>
<p>Instead of diving into code, it’s more important that our intuition about what neural networks are doing is as good as possible. The best (and most fun) way to do that is to play around and with things a bit, so go familiarize yourself with the <a href="https://playground.tensorflow.org/">Tensorflow Playground</a>, slide some knobs and pull some levers. The example in the lecture uses the same idea for demonstrating the intuition of neural networks.</p>
<blockquote class="blockquote">
<p><strong>Exercise 5.1</strong></p>
<p>Using the dataset with the two point clouds, create the minimal neural network that separates the clusters. You can share your answer with a link (the URL on playground.tensorflow.org changes as you update the network, so at any time you can use the link to show others what you have created).</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 5.2</strong></p>
<p>Using the dataset with the two circular clusters, one inner and one outer. Create the minimal neural network that separates the clusters.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Exercise 5.3 (OPTIONAL)</strong></p>
<p>See if you can create a network that performs well on the the dataset with the intertwined spirals. Can you do it with only <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>? &gt; <em>Hints</em>: &gt; &gt; Try experimenting with depth of the network, regularization and possibly the activation function</p>
</blockquote>
<p>Having now slid some knobs and pulled some levers to get some intuition for how the neural networks operate, we turn to the Multilayer Perceptron in <code>sklearn</code>.</p>
<blockquote class="blockquote">
<p><strong>Exercise 5.4 (OPTIONAL)</strong></p>
<p>Try to create a neural network which performs better than the best model on the test data. You may want to consider looking at different strengths of regularization (<code>alpha</code>, perhaps using <code>np.logspace</code>) and different amounts of hidden layers and hidden neurons. At this point in time, a just semi-exhaustive search of hyperparameters becomes computationally infeasible, and machine learning turns to art.</p>
<p><em>Note:</em> It is not given that a neural network performs best for the given problem, and even if the model exists, it may be hard to find the right architecture. I have not succeeded.</p>
<blockquote class="blockquote">
<p><em>Hints:</em></p>
<p>It may be time-consuming to do k fold cross validation. Splitting your development data into a train and validation set a single time is also a possibility. Only rule is that you don’t use the test data for model selection!</p>
</blockquote>
</blockquote>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>